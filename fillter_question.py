import pandas as pd
from openai import OpenAI
import json
import re
import os
from tqdm import tqdm

# ==============================================================================
# 1. C·∫§U H√åNH
# ==============================================================================
# K·∫øt n·ªëi LM Studio
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

# ƒê∆∞·ªùng d·∫´n file
NODES_FILE = 'Eval/import/data/neo4j_nodes.csv'
RELS_FILE = 'Eval/import/data/neo4j_relationships.csv'
INPUT_QA_FILE = 'Eval/data/medquad.csv' 
OUTPUT_FILE = 'Eval/data/advanced_filtered_qa.csv'

# C·∫•u h√¨nh l·ªçc
MAX_QUESTIONS = 700  # S·ªë l∆∞·ª£ng c√¢u h·ªèi t·ªëi ƒëa mu·ªën l·∫•y
MIN_NODE_LENGTH = 4  # Ch·ªâ l·∫•y t·ª´ kh√≥a d√†i > 3 k√Ω t·ª± ƒë·ªÉ tr√°nh nhi·ªÖu

# ==============================================================================
# 2. CHU·∫®N B·ªä D·ªÆ LI·ªÜU T·ª™ KG (NODES & RELATIONSHIPS)
# ==============================================================================
print("‚è≥ [B∆∞·ªõc 1] ƒêang x√¢y d·ª±ng b·ªô t·ª´ ƒëi·ªÉn t·ª´ Knowledge Graph...")

try:
    # --- ƒê·ªçc file ---
    nodes_df = pd.read_csv(NODES_FILE)
    rels_df = pd.read_csv(RELS_FILE)

    # --- 1. X√°c ƒë·ªãnh c√°c Node c√≥ quan h·ªá (Connected Nodes) ---
    # L·∫•y t·∫≠p h·ª£p t·∫•t c·∫£ ID xu·∫•t hi·ªán ·ªü c·ªôt START ho·∫∑c END trong file Relationships
    connected_ids = set(rels_df[':START_ID']).union(set(rels_df[':END_ID']))
    
    print(f"   - T·ªïng s·ªë Nodes g·ªëc: {len(nodes_df)}")
    print(f"   - S·ªë Nodes c√≥ quan h·ªá (ƒë∆∞·ª£c gi·ªØ l·∫°i): {len(connected_ids)}")

    # --- 2. X·ª≠ l√Ω Nodes ---
    # L√†m s·∫°ch t√™n node
    nodes_df['clean_name'] = nodes_df['name'].astype(str).str.replace(r'^\[Event:\s*|\]$', '', regex=True).str.lower().str.strip()
    
    # T·∫°o map ID -> T√™n (d√πng cho vi·ªác t·∫°o Pair ·ªü d∆∞·ªõi)
    id_to_name = dict(zip(nodes_df[':ID'], nodes_df['clean_name']))
    
    # T·∫†O T·∫¨P T·ª™ KH√ìA (CH·ªà L·∫§Y NODE C√ì QUAN H·ªÜ)
    kg_keywords = set()
    for _, row in nodes_df.iterrows():
        # LOGIC M·ªöI: Ch·ªâ th√™m v√†o t·ª´ ƒëi·ªÉn n·∫øu ID n·∫±m trong danh s√°ch connected_ids
        if row[':ID'] in connected_ids:
            name = row['clean_name']
            if len(name) >= MIN_NODE_LENGTH:
                kg_keywords.add(name)

    # --- 3. X·ª≠ l√Ω Relationships ---
    kg_pairs = []
    for _, row in rels_df.iterrows():
        start_name = id_to_name.get(row[':START_ID'])
        end_name = id_to_name.get(row[':END_ID'])
        
        # Ch·ªâ l·∫•y c·∫∑p quan h·ªá n·∫øu c·∫£ 2 ƒë·ªÅu c√≥ t√™n h·ª£p l·ªá
        if start_name and end_name and len(start_name) >= MIN_NODE_LENGTH and len(end_name) >= MIN_NODE_LENGTH:
            kg_pairs.append((start_name, end_name))

    print(f"‚úÖ D·ªØ li·ªáu KG sau l·ªçc: {len(kg_keywords)} t·ª´ kh√≥a (ch·ªâ nodes c√≥ qh·ªá), {len(kg_pairs)} c·∫∑p quan h·ªá.")

except Exception as e:
    print(f"‚ùå L·ªói ƒë·ªçc file KG: {e}")
    exit()

# ==============================================================================
# 3. B·ªò L·ªåC TH√î (SCORING & RANKING)
# ==============================================================================
print(f"\n‚è≥ [B∆∞·ªõc 2] ƒêang ch·∫•m ƒëi·ªÉm ƒë·ªô ph√π h·ª£p c·ªßa c√¢u h·ªèi trong {INPUT_QA_FILE}...")

try:
    qa_df = pd.read_csv(INPUT_QA_FILE)
    # T√¨m c·ªôt
    q_col = next((c for c in qa_df.columns if 'question' in c.lower()), qa_df.columns[0])
    a_col = next((c for c in qa_df.columns if 'answer' in c.lower()), qa_df.columns[1] if len(qa_df.columns)>1 else None)
except Exception as e:
    print(f"‚ùå L·ªói ƒë·ªçc file QA: {e}")
    exit()

def calculate_relevance_score(text):
    if not isinstance(text, str): return 0, ""
    text_lower = text.lower()
    score = 0
    reason = ""

    # Ti√™u ch√≠ 1: Ch·ª©a C·∫∂P QUAN H·ªÜ (Strong Match) - 10 ƒëi·ªÉm
    # ∆Øu ti√™n cao nh·∫•t v√¨ KG ch·∫Øc ch·∫Øn c√≥ th√¥ng tin li√™n k·∫øt
    for start, end in kg_pairs:
        if start in text_lower and end in text_lower:
            return 10, f"Strong Match: '{start}' & '{end}'" # Return lu√¥n ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô

    # Ti√™u ch√≠ 2: Ch·ª©a NODE (Weak Match) - 1 ƒëi·ªÉm
    # Duy·ªát qua keywords
    for k in kg_keywords:
        # Th√™m space ƒë·ªÉ tr√°nh match 1 ph·∫ßn t·ª´ (vd: 'flu' trong 'influence')
        if f" {k} " in f" {text_lower} ":
            return 1, f"Keyword Match: '{k}'"
            
    return 0, ""

# √Åp d·ª•ng ch·∫•m ƒëi·ªÉm (D√πng tqdm)
tqdm.pandas()
qa_df[['relevance_score', 'match_reason']] = qa_df[q_col].progress_apply(lambda x: pd.Series(calculate_relevance_score(x)))

# L·∫•y c√°c ·ª©ng vi√™n: ƒêi·ªÉm cao tr∆∞·ªõc, sau ƒë√≥ ƒë·∫øn ƒëi·ªÉm th·∫•p
candidates = qa_df[qa_df['relevance_score'] > 0].sort_values(by='relevance_score', ascending=False)

print(f"‚úÖ [B∆∞·ªõc 2 Xong] T√¨m th·∫•y {len(candidates)} c√¢u h·ªèi ti·ªÅm nƒÉng.")
print(f"   - Strong matches (Score 10): {len(candidates[candidates['relevance_score'] == 10])}")
print(f"   - Weak matches (Score 1): {len(candidates[candidates['relevance_score'] == 1])}")

# ==============================================================================
# 4. B·ªò L·ªåC TINH (SEMANTIC CHECK B·∫∞NG LLAMA 3.1)
# ==============================================================================
# Ch·ªâ l·∫•y top N c√¢u h·ªèi t·ªët nh·∫•t ƒë·ªÉ check b·∫±ng AI (ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian)
candidates_to_process = candidates.head(MAX_QUESTIONS).copy()

print(f"\n‚è≥ [B∆∞·ªõc 3] D√πng Llama 3.1 ki·ªÉm tra ng·ªØ nghƒ©a {len(candidates_to_process)} c√¢u h·ªèi t·ªët nh·∫•t...")

final_results = []

def is_invalid_answer(answer):
    """Lo·∫°i answer n·∫øu tr·ªëng ho·∫∑c b·∫Øt ƒë·∫ßu b·∫±ng Key Points"""
    if not isinstance(answer, str):
        return True
    clean = answer.strip().lower()
    if clean == "":
        return True
    if clean.startswith("key points"):
        return True
    return False

def verify_relevance_with_llm(question, reason):
    # Prompt th√¥ng minh h∆°n: Y√™u c·∫ßu AI ƒë√≥ng vai chuy√™n gia ƒë√°nh gi√°
    prompt = f"""
    Task: Verify if the Question is medically relevant to the extracted Concept/Context from our Database.

    Context from Database: {reason}
    Question: "{question}"

    Analyze: Does the question meaningfully ask about the medical concepts identified in the Context?
    Return strictly JSON: {{"is_relevant": true}} or {{"is_relevant": false}}
    """
    
    try:
        response = client.chat.completions.create(
            model="local-model",
            messages=[
                {"role": "system", "content": "You are a strict medical data validator. Output JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
        )
        content = response.choices[0].message.content
        if '"is_relevant": true' in content.lower():
            return True
        return False
    except:
        return False # M·∫∑c ƒë·ªãnh b·ªè qua n·∫øu l·ªói

for index, row in tqdm(candidates_to_process.iterrows(), total=len(candidates_to_process)):
    question = row[q_col]
    answer = row[a_col] if a_col else ""
    reason = row['match_reason']

    # ‚ùó B·ªî SUNG: lo·∫°i n·∫øu answer tr·ªëng ho·∫∑c b·∫Øt ƒë·∫ßu b·∫±ng Key Points
    if is_invalid_answer(answer):
        continue
    
    # Check b·∫±ng AI
    if verify_relevance_with_llm(question, reason):
        final_results.append({
            "Question": question,
            "Answer": answer,
            "Match_Type": "Strong" if row['relevance_score'] == 10 else "Weak",
            "Match_Detail": reason
        })



# ==============================================================================
# 5. L∆ØU K·∫æT QU·∫¢
# ==============================================================================
if final_results:
    # 1. T·∫°o DataFrame t·∫°m t·ª´ k·∫øt qu·∫£ AI check
    df_temp = pd.DataFrame(final_results)
    
    print(f"\n‚è≥ [B∆∞·ªõc B·ªï sung] ƒêang l·ªçc c√°c c√¢u tr·∫£ l·ªùi l·ªói tr√¨nh b√†y (format)...")
    
    # 2. ƒê·ªãnh nghƒ©a c√°c m·∫´u l·ªói (Abnormal patterns)
    # - Ch·ª©a k√Ω t·ª± Tab (\t)
    mask_tab = df_temp['Answer'].str.contains('\t', na=False)
    # - Xu·ªëng d√≤ng (\n) theo sau l√† h∆°n 4 kho·∫£ng tr·∫Øng (l·ªói th√≤ th·ª•t d√≤ng)
    mask_weird_spacing = df_temp['Answer'].str.contains(r'\n\s{4,}', regex=True, na=False)
    # - D√≠nh c√¢u (d·∫•u ch·∫•m li·ªÅn k·ªÅ ch·ªØ Hoa): vd "end.The"
    mask_glued = df_temp['Answer'].str.contains(r'(?<=[a-z])\.[A-Z]', regex=True, na=False)

    # 3. Gom t·∫•t c·∫£ l·ªói l·∫°i
    mask_to_remove = mask_tab | mask_weird_spacing | mask_glued
    
    # 4. L·ªçc b·ªè v√† gi·ªØ l·∫°i d·ªØ li·ªáu s·∫°ch
    df_out = df_temp[~mask_to_remove].copy() # <--- QUAN TR·ªåNG: L·∫•y ph·∫ßn b√π (~) c·ªßa l·ªói

    print(f"   - T·ªïng s·ªë c√¢u sau AI check: {len(df_temp)}")
    print(f"   - ƒê√£ lo·∫°i b·ªè: {mask_to_remove.sum()} c√¢u b·ªã l·ªói format.")
    print(f"   - C√≤n l·∫°i: {len(df_out)} c√¢u h·ªèi s·∫°ch.")

    # 5. L∆∞u file (Ch·ªâ l∆∞u n·∫øu c√≤n d·ªØ li·ªáu)
    if not df_out.empty:
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
        
        # Ch·ªâ l∆∞u 2 c·ªôt ch√≠nh
        df_save = df_out[['Question', 'Answer']]
        df_save.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
        
        print(f"\nüéâ HO√ÄN T·∫§T! ƒê√£ l·ªçc ƒë∆∞·ª£c {len(df_out)} c√¢u h·ªèi ch·∫•t l∆∞·ª£ng cao.")
        print(f"üíæ File l∆∞u t·∫°i: {OUTPUT_FILE}")
        
        # Th·ªëng k√™
        print("üîç Th·ªëng k√™ lo·∫°i Match:")
        if 'Match_Type' in df_out.columns:
            print(df_out['Match_Type'].value_counts())
        
        print("\nüîç 5 V√≠ d·ª• ƒë·∫ßu ti√™n:")
        # Ch·ªâ hi·ªán c·ªôt Match_Detail n·∫øu n√≥ t·ªìn t·∫°i ƒë·ªÉ debug
        cols_to_show = ['Question', 'Match_Detail'] if 'Match_Detail' in df_out.columns else ['Question']
        print(df_out[cols_to_show].head())
    else:
        print("\n‚ö† T·∫•t c·∫£ c√¢u h·ªèi ƒë√£ b·ªã lo·∫°i b·ªè b·ªüi b·ªô l·ªçc format (Tab/Spacing/Glued words).")

else:
    print("\n‚ö† Kh√¥ng c√≥ c√¢u h·ªèi n√†o v∆∞·ª£t qua b√†i ki·ªÉm tra ng·ªØ nghƒ©a (AI Check).")